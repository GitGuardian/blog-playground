{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 tips to Optimize your Django queries with PostgreSQL\n",
    "\n",
    "Welcome to this playground. It follows our [10 tips to Optimize your Django queries with PostgreSQL](https://www.gitguardian.com) blog article and allows you to test all given tips by yourself and to experiment your own optimization ideas.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup your project\n",
    "\n",
    "### Imports and Django setup\n",
    "\n",
    "You must run this cell each time your restart the kernel:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# setup django\n",
    "import django_init\n",
    "from django.contrib.postgres.aggregates import ArrayAgg\n",
    "from django.core.management import call_command\n",
    "from django.db import connection, reset_queries\n",
    "from django.db.models import Prefetch\n",
    "\n",
    "from books.models import Person, Book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Migrate your database\n",
    "\n",
    "Following cell allows you to migrate your database. You only need to run it if you change your Django models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create missing migrations\n",
    "call_command(\"makemigrations\", interactive=True)\n",
    "# Run migrations\n",
    "call_command(\"migrate\", interactive=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate your database\n",
    "\n",
    "Following cells populate the database with a lot of fake data. In case `autovacuum` is not set on your database instance, you'll also need to refresh your tables statistics in order to allow PostgreSQL queries planner to make the right decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_command(\"generate_data\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're making sure that statistics are up to date on all tables we'll use in this Notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"VACUUM ANALYSE books_book\")\n",
    "    cursor.execute(\"VACUUM ANALYSE books_person\")\n",
    "    cursor.execute(\"VACUUM ANALYSE books_book_readers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Person.objects.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Book.objects.count()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Good Method To Iterate fast\n",
    "\n",
    "Django natively proposes a convenient way to display SQL queries that are executed and to explain how they are resolved by the PostgreSQL query planner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Query:  SELECT \"books_person\".\"id\" FROM \"books_person\" LIMIT 10\n",
      "PostgreSQL query:  {'sql': 'SELECT \"books_person\".\"id\" FROM \"books_person\" ORDER BY \"books_person\".\"id\" ASC LIMIT 1', 'time': '0.011'}\n",
      "PostgreSQL explain analyze: Limit  (cost=0.29..0.55 rows=10 width=8) (actual time=0.073..0.086 rows=10 loops=1)\n",
      "  ->  Index Only Scan using books_person_pkey on books_person  (cost=0.29..536.83 rows=20100 width=8) (actual time=0.072..0.083 rows=10 loops=1)\n",
      "        Heap Fetches: 8\n",
      "Planning Time: 0.233 ms\n",
      "Execution Time: 0.112 ms\n"
     ]
    }
   ],
   "source": [
    "reset_queries()\n",
    "\n",
    "query_set = Person.objects.only(\"id\")\n",
    "person = query_set.first()\n",
    "\n",
    "print(\"SQL Query: \", query_set[:10].query)\n",
    "print(\"PostgreSQL query: \", connection.queries[0])  # needs DEBUG=True\n",
    "print(\"PostgreSQL explain analyze:\", query_set[:10].explain(ANALYZE=True))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Only What You Need\n",
    "\n",
    "You can significantly improve performances by reducing the amount of data sent to / from the database.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching using a large query\n",
    "\n",
    "The following query will be huge as it contains 100,000 email addresses. Even if execution time is fast, the total time (including Django processing and networking) is very long.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<QuerySet ['100038_howardmichael@foster.com', '100049_anthony80@payne.com', '100141_williamsjames@powell.org', '100170_susan73@chapman.com', '100208_ochoi@white.com', '100233_keithtorres@hess.com', '100362_steven52@cole.com', '100365_geraldmitchell@hernandez.org', '100444_moorekristina@anthony.com', '100468_robert79@hendrix.com']>\n",
      "PostgreSQL query:  SELECT \"books_person\".\"id\", \"books_person\".\"email\", \"books_person\".\"name\", \"books_person\".\"bio\" FROM \"books_person\" WHERE \"books_person\".\"email\" IN (SELECT U0.\"email\" FROM \"books_person\" U0 LIMIT 1000\n",
      "PostgreSQL explain analyze:  Limit  (cost=1245.32..1253.02 rows=10 width=781) (actual time=43.214..43.353 rows=10 loops=1)\n",
      "  ->  Nested Loop  (cost=1245.32..16709.54 rows=20100 width=781) (actual time=43.212..43.349 rows=10 loops=1)\n",
      "        ->  HashAggregate  (cost=1245.04..1446.04 rows=20100 width=31) (actual time=43.152..43.159 rows=10 loops=1)\n",
      "              Group Key: u0.email\n",
      "              Batches: 1  Memory Usage: 2577kB\n",
      "              ->  Limit  (cost=0.29..993.79 rows=20100 width=31) (actual time=0.023..29.450 rows=20100 loops=1)\n",
      "                    ->  Index Only Scan using books_person_email_key on books_person u0  (cost=0.29..993.79 rows=20100 width=31) (actual time=0.022..27.195 rows=20100 loops=1)\n",
      "                          Heap Fetches: 31\n",
      "        ->  Index Scan using books_person_email_key on books_person  (cost=0.29..0.75 rows=1 width=781) (actual time=0.017..0.017 rows=1 loops=10)\n",
      "              Index Cond: (email = u0.email)\n",
      "Planning Time: 5.030 ms\n",
      "Execution Time: 44.274 ms\n",
      "Total time: 0.05s\n"
     ]
    }
   ],
   "source": [
    "all_persons_qs = Person.objects.all()\n",
    "\n",
    "lots_emails = all_persons_qs.values_list(\"email\", flat=True)[:100_000]\n",
    "print(lots_emails[:10])\n",
    "\n",
    "big_qs = Person.objects.filter(email__in=lots_emails)\n",
    "\n",
    "reset_queries()\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "all_persons = big_qs.all()\n",
    "\n",
    "print(\"PostgreSQL query: \", str(big_qs.query)[:200])\n",
    "print(\"PostgreSQL explain analyze: \", big_qs[:10].explain(ANALYZE=True))\n",
    "print(f\"Total time: { time.perf_counter() - start_time:.2f}s\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching all the model\n",
    "\n",
    "In this example, we fetch all fields of the Person model, including `bio` (text).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL query:  SELECT \"books_person\".\"id\", \"books_person\".\"email\", \"books_person\".\"name\", \"books_person\".\"bio\" FROM \"books_person\"\n",
      "PostgreSQL explain analyze:  Limit  (cost=0.00..1.13 rows=10 width=781) (actual time=0.013..0.018 rows=10 loops=1)\n",
      "  ->  Seq Scan on books_person  (cost=0.00..2265.00 rows=20100 width=781) (actual time=0.011..0.014 rows=10 loops=1)\n",
      "Planning Time: 0.079 ms\n",
      "Execution Time: 0.037 ms\n",
      "Total time: 0.00s\n"
     ]
    }
   ],
   "source": [
    "reset_queries()\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "all_persons = all_persons_qs.all()\n",
    "\n",
    "print(\"PostgreSQL query: \", all_persons_qs.query)\n",
    "print(\"PostgreSQL explain analyze: \", all_persons_qs[:10].explain(ANALYZE=True))\n",
    "print(f\"Total time: { time.perf_counter() - start_time:.2f}s\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching only the id\n",
    "\n",
    "Getting only the `id` will improve the execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL query:  SELECT \"books_person\".\"id\" FROM \"books_person\"\n",
      "PostgreSQL explain analyze:  Limit  (cost=0.29..0.55 rows=10 width=8) (actual time=0.075..0.085 rows=10 loops=1)\n",
      "  ->  Index Only Scan using books_person_pkey on books_person  (cost=0.29..536.83 rows=20100 width=8) (actual time=0.073..0.081 rows=10 loops=1)\n",
      "        Heap Fetches: 8\n",
      "Planning Time: 0.088 ms\n",
      "Execution Time: 0.147 ms\n",
      "Total time: 0.00s\n"
     ]
    }
   ],
   "source": [
    "all_persons_qs = all_persons_qs.only(\"id\")\n",
    "\n",
    "reset_queries()\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "all_persons = all_persons_qs.all()\n",
    "\n",
    "print(\"PostgreSQL query: \", all_persons_qs.query)\n",
    "print(\"PostgreSQL explain analyze: \", all_persons_qs[:10].explain(ANALYZE=True))\n",
    "print(f\"Total time: { time.perf_counter() - start_time:.2f}s\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if you only need a list of ids, you can save a lot of time by using `values()` or `values_list()` and bypass full model instanciation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_persons_qs = all_persons_qs.only(\"id\").values_list(\"id\")\n",
    "\n",
    "reset_queries()\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "all_persons = all_persons_qs.all()\n",
    "\n",
    "print(\"PostgreSQL query: \", all_persons_qs.query)\n",
    "print(\"PostgreSQL explain analyze: \", all_persons_qs[:10].explain(ANALYZE=True))\n",
    "print(f\"Total time: { time.perf_counter() - start_time:.2f}s\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index what's you searching for\n",
    "\n",
    "Let's search authors by name.\n",
    "\n",
    "### Search without index\n",
    "\n",
    "Without any index, the request will scan all the table for the right value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"DROP INDEX IF EXISTS books_person_name_upper_idx\")\n",
    "    cursor.execute(\"DROP INDEX IF EXISTS books_person_name_idx\")\n",
    "\n",
    "tolstoy_qs = Person.objects.filter(name__iexact=\"tolstoy\").only(\"email\")\n",
    "\n",
    "reset_queries()\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "tolstoy = tolstoy_qs.all()\n",
    "\n",
    "print(\"PostgreSQL query: \", tolstoy_qs.query)\n",
    "print(\"PostgreSQL explain analyze: \", tolstoy_qs.explain(ANALYZE=True))\n",
    "print(f\"Total time: { time.perf_counter() - start_time:.2f}s\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search with a regular index\n",
    "\n",
    "The following code will create a regular index, just like Django would do if we added `index=True` to the `name` field. But B-Tree indexes are not able to perform case insensitive search, so the planner will still have to perform a full scan of the table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\n",
    "        \"CREATE INDEX IF NOT EXISTS books_person_name_idx ON books_person (name);\"\n",
    "    )\n",
    "\n",
    "# wait for the index creation\n",
    "time.sleep(5)\n",
    "\n",
    "tolstoy_qs = Person.objects.filter(name__iexact=\"tolstoy\").only(\"email\")\n",
    "\n",
    "reset_queries()\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "tolstoy = tolstoy_qs.all()\n",
    "\n",
    "print(\"PostgreSQL query: \", tolstoy_qs.query)\n",
    "print(\"PostgreSQL explain analyze: \", tolstoy_qs.explain(ANALYZE=True))\n",
    "print(f\"Total time: { time.perf_counter() - start_time:.2f}s\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search with case insensitive index\n",
    "\n",
    "Previous try was not a success, so let's try with a case insensitive index:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\n",
    "        \"CREATE INDEX IF NOT EXISTS books_person_name_upper_idx ON books_person (UPPER(name));\"\n",
    "    )\n",
    "\n",
    "# wait for the index creation\n",
    "time.sleep(5)\n",
    "\n",
    "tolstoy_qs = Person.objects.filter(name__iexact=\"tolstoy\").only(\"email\")\n",
    "\n",
    "reset_queries()\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "tolstoy = tolstoy_qs.all()\n",
    "\n",
    "print(\"PostgreSQL query: \", tolstoy_qs.query)\n",
    "print(\"PostgreSQL explain analyze: \", tolstoy_qs.explain(ANALYZE=True))\n",
    "print(f\"Total time: { time.perf_counter() - start_time:.2f}s\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select_related and prefetch_related are not always the best match\n",
    "\n",
    "We want to get the author of a list of N books.\n",
    "\n",
    "### Naive approach\n",
    "\n",
    "With the naive method, we need N+1 queries to achieve that:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "\n",
    "reset_queries()\n",
    "\n",
    "for book in Book.objects.all()[:N]:\n",
    "    author = book.author\n",
    "\n",
    "print(connection.queries)\n",
    "print(f\"{len(connection.queries)} queries have been executed\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select_related()\n",
    "\n",
    "Using `select_related()` only 1 query is needed:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_queries()\n",
    "\n",
    "for book in Book.objects.select_related(\"author\")[:10]:\n",
    "    author = book.author\n",
    "\n",
    "print(connection.queries)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using prefetch_related for \"\\* to many\" relations\n",
    "\n",
    "For OneToMany of ManyToMany relations, `prefetch_related()` comes in handy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_queries()\n",
    "\n",
    "for person in Person.objects.prefetch_related(\"writings\")[:10]:\n",
    "    writings = person.writings\n",
    "\n",
    "print(connection.queries)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it can generate huge queries which will may be long to execute:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_queries()\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "result = {}\n",
    "for person in Person.objects.prefetch_related(\"writings\")[:100_000]:\n",
    "    result[person.email] = [book.title for book in person.writings.all()]\n",
    "\n",
    "print(\"Query duration:\", sum(float(query[\"time\"]) for query in connection.queries))\n",
    "print(f\"Total time: { time.perf_counter() - start_time:.2f}s\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using to_attr to speed up prefetch_related\n",
    "\n",
    "As stated in Django's [prefetch_related documentation](https://docs.djangoproject.com/en/4.1/ref/models/querysets/#prefetch-related) you can use `to_attr` to store cached results to a list. It doesn't help much on query duration, but the total time is much better:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_queries()\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "result = {\n",
    "    person.email: [book.title for book in person.prefetched_writings]\n",
    "    for person in Person.objects.prefetch_related(\n",
    "        Prefetch(\"writings\", to_attr=\"prefetched_writings\")\n",
    "    )[:100_000]\n",
    "}\n",
    "\n",
    "print(\"Query duration\", sum(float(query[\"time\"]) for query in connection.queries))\n",
    "print(f\"Total time: { time.perf_counter() - start_time:.2f}s\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using aggregation\n",
    "\n",
    "Another solution to get our book titles is to use aggration. Again, we can see performance gains:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_queries()\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "result = {\n",
    "    person.email: person.writings_titles\n",
    "    for person in Person.objects.annotate(writings_titles=ArrayAgg(\"writings__title\"))[\n",
    "        :100_000\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(connection.queries[-1][\"sql\"])\n",
    "print(\"Query duration\", sum(float(query[\"time\"]) for query in connection.queries))\n",
    "print(f\"Total time: { time.perf_counter() - start_time:.2f}s\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't need to instanciate Models but just some values, we can save even more time:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_queries()\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "result = {\n",
    "    person.email: person.writings_titles\n",
    "    for person in Person.objects.annotate(\n",
    "        writings_titles=ArrayAgg(\"writings__title\")\n",
    "    ).values_list(\"email\", \"writings_titles\", named=True)[:100_000]\n",
    "}\n",
    "\n",
    "print(connection.queries)\n",
    "print(\"Query duration\", sum(float(query[\"time\"]) for query in connection.queries))\n",
    "print(f\"Total time: { time.perf_counter() - start_time:.2f}s\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregations VS subqueries\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get writers stats using aggregations\n",
    "\n",
    "We want to get the list of books written by an author, and the total count of readers.\n",
    "With Django ORM, this is usually achieved using `annotate()` method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from django.db.models import Count\n",
    "\n",
    "writers_stats_qs = Person.objects.annotate(\n",
    "    writings_title=ArrayAgg(\"writings__title\"),\n",
    "    readers_count=Count(\"writings__readers\"),\n",
    ").values_list(\"name\", \"bio\", \"writings_title\", \"readers_count\")\n",
    "\n",
    "reset_queries()\n",
    "\n",
    "writers_stats = writers_stats_qs[::1]\n",
    "\n",
    "print(connection.queries)\n",
    "print(\"query duration\", sum(float(query[\"time\"]) for query in connection.queries))\n",
    "print(\"pg explain analyze:\", writers_stats_qs.explain(ANALYZE=True))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get writers stats using subqueries\n",
    "\n",
    "The following example will use 2 subqueries instead of `annotate()` for the same purpose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from django.db.models import Count, OuterRef\n",
    "from django.contrib.postgres.expressions import ArraySubquery\n",
    "\n",
    "writings_subquery = Book.objects.filter(author_id=OuterRef(\"id\")).values(\"title\")\n",
    "readers_subquery = (\n",
    "    Book.objects.filter(author_id=OuterRef(\"id\"))\n",
    "    .values(\"author_id\")\n",
    "    .values(count=Count(\"readers__id\"))[:1]\n",
    ")\n",
    "writers_stats_qs = Person.objects.annotate(\n",
    "    writings_title=ArraySubquery(writings_subquery), readers_count=readers_subquery\n",
    ").values_list(\"name\", \"bio\", \"writings_title\", \"readers_count\")\n",
    "\n",
    "reset_queries()\n",
    "\n",
    "writers_stats = writers_stats_qs[::1]\n",
    "\n",
    "print(connection.queries)\n",
    "print(\"query duration\", sum(float(query[\"time\"]) for query in connection.queries))\n",
    "print(\"pg explain analyze:\", writers_stats_qs.explain(ANALYZE=True))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Your RAM\n",
    "\n",
    "We first need a small lib `psutil` to measure the RAM consumed by our code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from time import sleep\n",
    "\n",
    "import psutil\n",
    "\n",
    "\n",
    "def measure_ram_consumption(function_to_audit):\n",
    "    \"\"\"Output the RAM consumption of the function passed as parameter\"\"\"\n",
    "    initial_available_memory = psutil.virtual_memory().available\n",
    "    min_available_memory = initial_available_memory\n",
    "    is_running = True\n",
    "\n",
    "    class RamUsageThread(Thread):\n",
    "        def run(self) -> None:\n",
    "            nonlocal min_available_memory\n",
    "            while is_running:\n",
    "                min_available_memory = min(\n",
    "                    psutil.virtual_memory().available, min_available_memory\n",
    "                )\n",
    "                sleep(0.1)\n",
    "            return min_available_memory\n",
    "\n",
    "    ram_thread = RamUsageThread()\n",
    "    ram_thread.start()\n",
    "    function_to_audit()\n",
    "    is_running = False\n",
    "\n",
    "    print(\n",
    "        \"RAM consumption:\",\n",
    "        (initial_available_memory - min_available_memory) / 2**20,\n",
    "        \"MB\",\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate using the Queryset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_over_persons():\n",
    "    for person in Person.objects.all():\n",
    "        pass\n",
    "\n",
    "\n",
    "measure_ram_consumption(iter_over_persons)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate using an iterator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_over_persons_with_iterator():\n",
    "    for person in Person.objects.iterator():\n",
    "        pass\n",
    "\n",
    "\n",
    "measure_ram_consumption(iter_over_persons_with_iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "django-perfs-playground-tFy-cSse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
