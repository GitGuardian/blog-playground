{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 tips to Optimize your Django queries with PostgreSQL\n",
    "Welcome to this playground. It follows our [10 tips  to Optimize your Django queries with PostgreSQL](https://www.gitguardian.com) blog article and allows you to test all given tips by yourself and to experiment your own optimization ideas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup your project\n",
    "\n",
    "### Imports and Django setup\n",
    "You must run this cell each time your restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# setup django\n",
    "import django_init\n",
    "from django.contrib.postgres.aggregates import ArrayAgg\n",
    "from django.core.management import call_command\n",
    "from django.db import connection, reset_queries\n",
    "from django.db.models import Prefetch\n",
    "\n",
    "from books.models import Person, Book\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Migrate your database\n",
    "Following cell allows you to migrate your database. You only need to run it if you change your Django models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No changes detected\n",
      "Operations to perform:\n",
      "  Apply all migrations: admin, auth, books, contenttypes, sessions\n",
      "Running migrations:\n",
      "  No migrations to apply.\n"
     ]
    }
   ],
   "source": [
    "# Create missing migrations\n",
    "call_command(\"makemigrations\", interactive=True)\n",
    "# Run migrations\n",
    "call_command(\"migrate\", interactive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate your database\n",
    "Following cells populate the database with a lot of fake data. In case `autovacuum` is not set on your database instance, you'll also need to refresh your tables statistics in order to allow PostgreSQL queries planner to make the right decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_command(\"generate_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"VACUUM ANALYSE books_book\")\n",
    "    cursor.execute(\"VACUUM ANALYSE books_person\")\n",
    "    cursor.execute(\"VACUUM ANALYSE books_book_readers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999996"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Person.objects.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Book.objects.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Good Method To Iterate fast\n",
    "Django natively proposes convenient ways to display SQL queries that are executed and to explain how they are resolved by PostgreSQL query planner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Query SELECT \"books_person\".\"id\" FROM \"books_person\" LIMIT 10\n",
      "PostgreSQL query:  {'sql': 'SELECT \"books_person\".\"id\" FROM \"books_person\" ORDER BY \"books_person\".\"id\" ASC LIMIT 1', 'time': '0.002'}\n",
      "pg explain analyze: Limit  (cost=0.00..1.11 rows=10 width=8) (actual time=0.034..0.038 rows=10 loops=1)\n",
      "  ->  Seq Scan on books_person  (cost=0.00..113225.35 rows=1016335 width=8) (actual time=0.033..0.035 rows=10 loops=1)\n",
      "Planning Time: 0.064 ms\n",
      "Execution Time: 0.055 ms\n"
     ]
    }
   ],
   "source": [
    "reset_queries()\n",
    "\n",
    "query_set = Person.objects.only(\"id\")\n",
    "person = query_set.first()\n",
    "\n",
    "print(\"SQL Query: \", query_set[:10].query)\n",
    "print(\"PostgreSQL query: \", connection.queries[0])  # needs DEBUG=True\n",
    "print(\"PostgreSQL explain analyze:\", query_set[:10].explain(ANALYZE=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Only What You Need\n",
    "You can significantly improve performances by reducing the amount of data sent to / by database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching using a large query\n",
    "The following query will be huge as the query sent to PostgreSQL contains 100,000 email addresses. Even if the execution time is small, the total time (including Django processing and networking) is very long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<QuerySet ['1000000_christopher74@example.net', '1000000_daniel75@example.org', '1000000_wdean@example.com', '100000_mcooper@example.org', '100001_jeffreyortiz@example.com', '100001_mary83@example.com', '100002_garciadaniel@example.org', '100002_vmoore@example.net', '100003_amandadiaz@example.net', '100005_michelle84@example.net']>\n",
      "PostgreSQL query:  SELECT \"books_person\".\"id\", \"books_person\".\"email\", \"books_person\".\"name\", \"books_person\".\"bio\" FROM \"books_person\" WHERE \"books_person\".\"email\" IN (SELECT U0.\"email\" FROM \"books_person\" U0 LIMIT 1000\n",
      "PostgreSQL explain analyze: Limit  (cost=8612.44..8643.92 rows=10 width=780) (actual time=413.782..414.509 rows=10 loops=1)\n",
      "  ->  Hash Semi Join  (cost=8612.44..323361.89 rows=100000 width=780) (actual time=413.781..414.505 rows=10 loops=1)\n",
      "        Hash Cond: (books_person.email = u0.email)\n",
      "        ->  Seq Scan on books_person  (cost=0.00..113061.96 rows=999996 width=780) (actual time=0.202..1.438 rows=139 loops=1)\n",
      "        ->  Hash  (cost=6678.44..6678.44 rows=100000 width=29) (actual time=412.401..412.401 rows=100000 loops=1)\n",
      "              Buckets: 65536  Batches: 2  Memory Usage: 3514kB\n",
      "              ->  Limit  (cost=0.42..5678.44 rows=100000 width=29) (actual time=0.016..292.784 rows=100000 loops=1)\n",
      "                    ->  Index Only Scan using books_person_email_key on books_person u0  (cost=0.42..56780.37 rows=999996 width=29) (actual time=0.015..282.813 rows=100000 loops=1)\n",
      "                          Heap Fetches: 0\n",
      "Planning Time: 4.975 ms\n",
      "Execution Time: 416.331 ms\n",
      "Total time: 0.43s\n"
     ]
    }
   ],
   "source": [
    "all_persons_qs = Person.objects.all()\n",
    "\n",
    "lots_emails = all_persons_qs.values_list(\"email\", flat=True)[:100_000]\n",
    "print(lots_emails[:10])\n",
    "\n",
    "big_qs = Person.objects.filter(email__in=lots_emails)\n",
    "\n",
    "reset_queries()\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "all_persons = big_qs.all()\n",
    "\n",
    "print(\"PostgreSQL query: \", str(big_qs.query)[:200])\n",
    "print(\"PostgreSQL explain analyze: \", big_qs[:10].explain(ANALYZE=True))\n",
    "print(f\"Total time: { time.perf_counter() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching all the model\n",
    "In this example, we fetch all fields of the Person model, including `bio` (text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL query:  SELECT \"books_person\".\"id\", \"books_person\".\"email\", \"books_person\".\"name\", \"books_person\".\"bio\" FROM \"books_person\"\n",
      "PostgreSQL explain analyze: Limit  (cost=0.00..1.13 rows=10 width=780) (actual time=0.013..0.019 rows=10 loops=1)\n",
      "  ->  Seq Scan on books_person  (cost=0.00..113061.96 rows=999996 width=780) (actual time=0.012..0.016 rows=10 loops=1)\n",
      "Planning Time: 0.104 ms\n",
      "Execution Time: 0.036 ms\n",
      "Total time: 0.0017139300471171737\n"
     ]
    }
   ],
   "source": [
    "reset_queries()\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "all_persons = all_persons_qs.all()\n",
    "\n",
    "print(\"PostgreSQL query: \", all_persons_qs.query)\n",
    "print(\"PostgreSQL explain analyze: \", all_persons_qs[:10].explain(ANALYZE=True))\n",
    "print(f\"Total time: { time.perf_counter() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching only the id\n",
    "Getting only the `id` will improve the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL query:  SELECT \"books_person\".\"id\" FROM \"books_person\"\n",
      "PostgreSQL explain analyze: Limit  (cost=0.42..0.80 rows=10 width=8) (actual time=1.493..1.499 rows=10 loops=1)\n",
      "  ->  Index Only Scan using books_person_pkey on books_person  (cost=0.42..37496.37 rows=999996 width=8) (actual time=1.491..1.495 rows=10 loops=1)\n",
      "        Heap Fetches: 0\n",
      "Planning Time: 0.092 ms\n",
      "Execution Time: 1.522 ms\n",
      "Total time: 0.0039990650257095695\n"
     ]
    }
   ],
   "source": [
    "all_persons_qs = all_persons_qs.only(\"id\")\n",
    "\n",
    "reset_queries()\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "all_persons = all_persons_qs.all()\n",
    "\n",
    "print(\"PostgreSQL query: \", all_persons_qs.query)\n",
    "print(\"PostgreSQL explain analyze: \", all_persons_qs[:10].explain(ANALYZE=True))\n",
    "print(f\"Total time: { time.perf_counter() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if you only need a list of ids, you can save a lot of time not instanciating models by using `values()` or `values_list()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL query:  SELECT \"books_person\".\"id\" FROM \"books_person\"\n",
      "PostgreSQL explain analyze: Limit  (cost=0.42..0.80 rows=10 width=8) (actual time=0.017..0.022 rows=10 loops=1)\n",
      "  ->  Index Only Scan using books_person_pkey on books_person  (cost=0.42..37496.37 rows=999996 width=8) (actual time=0.016..0.020 rows=10 loops=1)\n",
      "        Heap Fetches: 0\n",
      "Planning Time: 0.085 ms\n",
      "Execution Time: 0.039 ms\n",
      "Total time: 0.004087422043085098\n"
     ]
    }
   ],
   "source": [
    "all_persons_qs = all_persons_qs.only(\"id\").values_list(\"id\")\n",
    "\n",
    "reset_queries()\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "all_persons = all_persons_qs.all()\n",
    "\n",
    "print(\"PostgreSQL query: \", all_persons_qs.query)\n",
    "print(\"PostgreSQL explain analyze: \", all_persons_qs[:10].explain(ANALYZE=True))\n",
    "print(f\"Total time: { time.perf_counter() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index what's you searching for\n",
    "Let's search authors by name.\n",
    "\n",
    "### Search without index\n",
    "Without any index, the request will scan all the table for the right value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL query:  SELECT \"books_person\".\"id\", \"books_person\".\"email\" FROM \"books_person\" WHERE UPPER(\"books_person\".\"name\"::text) = UPPER(tolstoy)\n",
      "PostgreSQL explain analyze: Gather  (cost=1000.00..110811.98 rows=5000 width=37) (actual time=583.833..586.294 rows=1 loops=1)\n",
      "  Workers Planned: 2\n",
      "  Workers Launched: 2\n",
      "  ->  Parallel Seq Scan on books_person  (cost=0.00..109311.98 rows=2083 width=37) (actual time=544.738..565.079 rows=0 loops=3)\n",
      "        Filter: (upper(name) = 'TOLSTOY'::text)\n",
      "        Rows Removed by Filter: 333332\n",
      "Planning Time: 2.016 ms\n",
      "Execution Time: 586.323 ms\n",
      "Total time: 0.5973122449358925\n"
     ]
    }
   ],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\"DROP INDEX IF EXISTS books_person_name_upper_idx\")\n",
    "    cursor.execute(\"DROP INDEX IF EXISTS books_person_name_idx\")\n",
    "\n",
    "tolstoy_qs = Person.objects.filter(name__iexact=\"tolstoy\").only(\"email\")\n",
    "\n",
    "reset_queries()\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "tolstoy = tolstoy_qs.all()\n",
    "\n",
    "print(\"PostgreSQL query: \", tolstoy_qs.query)\n",
    "print(\"PostgreSQL explain analyze: \", tolstoy_qs.explain(ANALYZE=True))\n",
    "print(f\"Total time: { time.perf_counter() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search with a regular index\n",
    "The following code will create a regular index just like Django would do if we add `index=True` to the `name` field. But B-Tree indexes are not able to perform case insensitive search and the planner will still full scan the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL query:  SELECT \"books_person\".\"id\", \"books_person\".\"email\" FROM \"books_person\" WHERE UPPER(\"books_person\".\"name\"::text) = UPPER(tolstoy)\n",
      "PostgreSQL explain analyze: Gather  (cost=1000.00..110811.98 rows=5000 width=37) (actual time=284.198..314.624 rows=1 loops=1)\n",
      "  Workers Planned: 2\n",
      "  Workers Launched: 2\n",
      "  ->  Parallel Seq Scan on books_person  (cost=0.00..109311.98 rows=2083 width=37) (actual time=294.664..303.912 rows=0 loops=3)\n",
      "        Filter: (upper(name) = 'TOLSTOY'::text)\n",
      "        Rows Removed by Filter: 333332\n",
      "Planning Time: 0.698 ms\n",
      "Execution Time: 314.645 ms\n",
      "Total time: 0.3327687010169029\n"
     ]
    }
   ],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\n",
    "        \"CREATE INDEX IF NOT EXISTS books_person_name_idx ON books_person (name);\"\n",
    "    )\n",
    "\n",
    "# wait for the index creation\n",
    "time.sleep(5)\n",
    "\n",
    "tolstoy_qs = Person.objects.filter(name__iexact=\"tolstoy\").only(\"email\")\n",
    "\n",
    "reset_queries()\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "tolstoy = tolstoy_qs.all()\n",
    "\n",
    "print(\"PostgreSQL query: \", tolstoy_qs.query)\n",
    "print(\"PostgreSQL explain analyze: \", tolstoy_qs.explain(ANALYZE=True))\n",
    "print(f\"Total time: { time.perf_counter() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search with case insensitive index\n",
    "Previous try was not a success, so we try again, but with a case insensitive index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PostgreSQL query:  SELECT \"books_person\".\"id\", \"books_person\".\"email\" FROM \"books_person\" WHERE UPPER(\"books_person\".\"name\"::text) = UPPER(tolstoy)\n",
      "PostgreSQL explain analyze: Bitmap Heap Scan on books_person  (cost=119.17..16534.54 rows=5000 width=37) (actual time=0.337..0.339 rows=1 loops=1)\n",
      "  Recheck Cond: (upper(name) = 'TOLSTOY'::text)\n",
      "  Heap Blocks: exact=1\n",
      "  ->  Bitmap Index Scan on books_person_name_upper_idx  (cost=0.00..117.92 rows=5000 width=0) (actual time=0.105..0.105 rows=1 loops=1)\n",
      "        Index Cond: (upper(name) = 'TOLSTOY'::text)\n",
      "Planning Time: 2.482 ms\n",
      "Execution Time: 0.739 ms\n",
      "Total time: 0.0077229730086401105\n"
     ]
    }
   ],
   "source": [
    "with connection.cursor() as cursor:\n",
    "    cursor.execute(\n",
    "        \"CREATE INDEX IF NOT EXISTS books_person_name_upper_idx ON books_person (UPPER(name));\"\n",
    "    )\n",
    "\n",
    "# wait for the index creation\n",
    "time.sleep(5)\n",
    "\n",
    "tolstoy_qs = Person.objects.filter(name__iexact=\"tolstoy\").only(\"email\")\n",
    "\n",
    "reset_queries()\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "tolstoy = tolstoy_qs.all()\n",
    "\n",
    "print(\"PostgreSQL query: \", tolstoy_qs.query)\n",
    "print(\"PostgreSQL explain analyze: \", tolstoy_qs.explain(ANALYZE=True))\n",
    "print(f\"Total time: { time.perf_counter() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select_related and prefetch_related are not always the best match\n",
    "We want to get the author of a list of N books.\n",
    "### Naive approach\n",
    "With the naive method, we need N+1 queries to achieve that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'sql': 'SELECT \"books_book\".\"id\", \"books_book\".\"title\", \"books_book\".\"author_id\" FROM \"books_book\" LIMIT 10', 'time': '0.002'}, {'sql': 'SELECT \"books_person\".\"id\", \"books_person\".\"email\", \"books_person\".\"name\", \"books_person\".\"bio\" FROM \"books_person\" WHERE \"books_person\".\"id\" = 1885800 LIMIT 21', 'time': '0.004'}, {'sql': 'SELECT \"books_person\".\"id\", \"books_person\".\"email\", \"books_person\".\"name\", \"books_person\".\"bio\" FROM \"books_person\" WHERE \"books_person\".\"id\" = 2257661 LIMIT 21', 'time': '0.003'}, {'sql': 'SELECT \"books_person\".\"id\", \"books_person\".\"email\", \"books_person\".\"name\", \"books_person\".\"bio\" FROM \"books_person\" WHERE \"books_person\".\"id\" = 1713720 LIMIT 21', 'time': '0.002'}, {'sql': 'SELECT \"books_person\".\"id\", \"books_person\".\"email\", \"books_person\".\"name\", \"books_person\".\"bio\" FROM \"books_person\" WHERE \"books_person\".\"id\" = 1621007 LIMIT 21', 'time': '0.001'}, {'sql': 'SELECT \"books_person\".\"id\", \"books_person\".\"email\", \"books_person\".\"name\", \"books_person\".\"bio\" FROM \"books_person\" WHERE \"books_person\".\"id\" = 1695429 LIMIT 21', 'time': '0.001'}, {'sql': 'SELECT \"books_person\".\"id\", \"books_person\".\"email\", \"books_person\".\"name\", \"books_person\".\"bio\" FROM \"books_person\" WHERE \"books_person\".\"id\" = 2262585 LIMIT 21', 'time': '0.005'}, {'sql': 'SELECT \"books_person\".\"id\", \"books_person\".\"email\", \"books_person\".\"name\", \"books_person\".\"bio\" FROM \"books_person\" WHERE \"books_person\".\"id\" = 1750979 LIMIT 21', 'time': '0.004'}, {'sql': 'SELECT \"books_person\".\"id\", \"books_person\".\"email\", \"books_person\".\"name\", \"books_person\".\"bio\" FROM \"books_person\" WHERE \"books_person\".\"id\" = 1624284 LIMIT 21', 'time': '0.002'}, {'sql': 'SELECT \"books_person\".\"id\", \"books_person\".\"email\", \"books_person\".\"name\", \"books_person\".\"bio\" FROM \"books_person\" WHERE \"books_person\".\"id\" = 1903161 LIMIT 21', 'time': '0.005'}, {'sql': 'SELECT \"books_person\".\"id\", \"books_person\".\"email\", \"books_person\".\"name\", \"books_person\".\"bio\" FROM \"books_person\" WHERE \"books_person\".\"id\" = 1396662 LIMIT 21', 'time': '0.004'}]\n",
      "11 queries have been executed\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "\n",
    "reset_queries()\n",
    "\n",
    "for book in Book.objects.all()[:N]:\n",
    "    author = book.author\n",
    "\n",
    "print(connection.queries)\n",
    "print(f\"{len(connection.queries)} queries have been executed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select_related()\n",
    "Using `select_related()` only 1 query is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'sql': 'SELECT \"books_book\".\"id\", \"books_book\".\"title\", \"books_book\".\"author_id\", \"books_person\".\"id\", \"books_person\".\"email\", \"books_person\".\"name\", \"books_person\".\"bio\" FROM \"books_book\" INNER JOIN \"books_person\" ON (\"books_book\".\"author_id\" = \"books_person\".\"id\") LIMIT 10', 'time': '0.004'}]\n"
     ]
    }
   ],
   "source": [
    "reset_queries()\n",
    "\n",
    "for book in Book.objects.select_related(\"author\")[:10]:\n",
    "    author = book.author\n",
    "\n",
    "print(connection.queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using prefetch_related for \"* to many\" relations \n",
    "For OneToMany of ManyToMany relations, `prefetch_related()` is used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'sql': 'SELECT \"books_person\".\"id\", \"books_person\".\"email\", \"books_person\".\"name\", \"books_person\".\"bio\" FROM \"books_person\" LIMIT 10', 'time': '0.001'}, {'sql': 'SELECT \"books_book\".\"id\", \"books_book\".\"title\", \"books_book\".\"author_id\" FROM \"books_book\" WHERE \"books_book\".\"author_id\" IN (2169230, 2169241, 2169248, 2169255, 2169264, 2169272, 2169277, 2169285, 2169294, 2169304)', 'time': '0.002'}]\n"
     ]
    }
   ],
   "source": [
    "reset_queries()\n",
    "\n",
    "for person in Person.objects.prefetch_related(\"writings\")[:10]:\n",
    "    writings = person.writings\n",
    "\n",
    "print(connection.queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it can generate huge queries which will may be long to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query duration 0.9329999999999999\n",
      "Total time: 17.908183124032803\n"
     ]
    }
   ],
   "source": [
    "reset_queries()\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "result = {}\n",
    "for person in Person.objects.prefetch_related(\"writings\")[:100_000]:\n",
    "    result[person.email] = [book.title for book in person.writings.all()]\n",
    "\n",
    "print(\"Query duration:\", sum(float(query[\"time\"]) for query in connection.queries))\n",
    "print(f\"Total time: { time.perf_counter() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using to_attr to speed up prefetch_related\n",
    "As stated in Django's [prefetch_related documentation](https://docs.djangoproject.com/en/4.1/ref/models/querysets/#prefetch-related) you can use `to_attr` to store cached results to a list. It doesn't help much on query duration, but the total time is much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query duration 1.059\n",
      "Total time: 3.41s\n"
     ]
    }
   ],
   "source": [
    "reset_queries()\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "result = {\n",
    "    person.email: [book.title for book in person.prefetched_writings]\n",
    "    for person in Person.objects.prefetch_related(\n",
    "        Prefetch(\"writings\", to_attr=\"prefetched_writings\")\n",
    "    )[:100_000]\n",
    "}\n",
    "\n",
    "print(\"Query duration\", sum(float(query[\"time\"]) for query in connection.queries))\n",
    "print(f\"Total time: { time.perf_counter() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using aggregation\n",
    "Another solution to get our book titles is to use aggration. Again, we can see performance gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT \"books_person\".\"id\", \"books_person\".\"email\", \"books_person\".\"name\", \"books_person\".\"bio\", ARRAY_AGG(\"books_book\".\"title\" ) AS \"writings_titles\" FROM \"books_person\" LEFT OUTER JOIN \"books_book\" ON (\"books_person\".\"id\" = \"books_book\".\"author_id\") GROUP BY \"books_person\".\"id\" LIMIT 100000\n",
      "Query duration 0.973\n",
      "Total time: 1.91s\n"
     ]
    }
   ],
   "source": [
    "reset_queries()\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "result = {\n",
    "    person.email: person.writings_titles\n",
    "    for person in Person.objects.annotate(writings_titles=ArrayAgg(\"writings__title\"))[\n",
    "        :100_000\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(connection.queries[-1]['sql'])\n",
    "print(\"Query duration\", sum(float(query[\"time\"]) for query in connection.queries))\n",
    "print(f\"Total time: { time.perf_counter() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't need to instanciate Models but just some values, we can same more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'sql': 'SELECT \"books_person\".\"email\", ARRAY_AGG(\"books_book\".\"title\" ) AS \"writings_titles\" FROM \"books_person\" LEFT OUTER JOIN \"books_book\" ON (\"books_person\".\"id\" = \"books_book\".\"author_id\") GROUP BY \"books_person\".\"id\" LIMIT 100000', 'time': '0.228'}]\n",
      "Query duration 0.228\n",
      "Total time: 0.38546457199845463\n"
     ]
    }
   ],
   "source": [
    "reset_queries()\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "result = {\n",
    "    person.email: person.writings_titles\n",
    "    for person in Person.objects.annotate(\n",
    "        writings_titles=ArrayAgg(\"writings__title\")\n",
    "    ).values_list(\"email\", \"writings_titles\", named=True)[:100_000]\n",
    "}\n",
    "\n",
    "print(connection.queries)\n",
    "print(\"Query duration\", sum(float(query[\"time\"]) for query in connection.queries))\n",
    "print(f\"Total time: { time.perf_counter() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregations VS subqueries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get writers stats using aggregations\n",
    "We want to get the list of books written by an author, and the total count of readers. \n",
    "With Django ORM, this is usually achieved using `annotate()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "query duration 0\n",
      "pg explain analyze: GroupAggregate  (cost=1.27..76855404.29 rows=999996 width=791) (actual time=1.513..61136.046 rows=999996 loops=1)\n",
      "  Group Key: books_person.id\n",
      "  ->  Merge Left Join  (cost=1.27..1423986.02 rows=10055855776 width=822) (actual time=1.494..53501.557 rows=21110086 loops=1)\n",
      "        Merge Cond: (books_person.id = books_book.author_id)\n",
      "        ->  Index Scan using books_person_pkey on books_person  (cost=0.42..258578.88 rows=999996 width=751) (actual time=0.013..2512.841 rows=999996 loops=1)\n",
      "        ->  Materialize  (cost=0.84..911509.75 rows=20111792 width=79) (actual time=1.477..43920.691 rows=20112082 loops=1)\n",
      "              ->  Nested Loop Left Join  (cost=0.84..861230.27 rows=20111792 width=79) (actual time=1.461..31093.646 rows=20112082 loops=1)\n",
      "                    ->  Index Scan using books_book_author_id_8b91747b on books_book  (cost=0.28..186.27 rows=2000 width=79) (actual time=0.509..9.167 rows=2000 loops=1)\n",
      "                    ->  Index Only Scan using books_book_readers_book_id_person_id_871b68d4_uniq on books_book_readers  (cost=0.56..327.75 rows=10277 width=16) (actual time=0.271..13.839 rows=10056 loops=2000)\n",
      "                          Index Cond: (book_id = books_book.id)\n",
      "                          Heap Fetches: 0\n",
      "Planning Time: 3.450 ms\n",
      "Execution Time: 61355.183 ms\n"
     ]
    }
   ],
   "source": [
    "from django.db.models import Count\n",
    "\n",
    "writers_stats_qs = Person.objects.annotate(\n",
    "    writings_title=ArrayAgg(\"writings__title\"),\n",
    "    readers_count=Count(\"writings__readers\"),\n",
    ").values_list(\"name\", \"bio\", \"writings_title\", \"readers_count\")\n",
    "\n",
    "reset_queries()\n",
    "\n",
    "writers_stats = writers_stats_qs.all()\n",
    "\n",
    "print(connection.queries)\n",
    "print(\"query duration\", sum(float(query[\"time\"]) for query in connection.queries))\n",
    "print(\"pg explain analyze:\", writers_stats_qs.explain(ANALYZE=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get writers stats using subqueries\n",
    "The following example will use 2 subqueries instead of `annotate()` for the same purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "query duration 0\n",
      "pg explain analyze: Seq Scan on books_person  (cost=0.00..527045954.22 rows=999996 width=783) (actual time=1.326..20453.472 rows=999996 loops=1)\n",
      "  SubPlan 1\n",
      "    ->  Index Scan using books_book_author_id_8b91747b on books_book u0  (cost=0.28..8.29 rows=1 width=63) (actual time=0.002..0.002 rows=0 loops=999996)\n",
      "          Index Cond: (author_id = books_person.id)\n",
      "  SubPlan 2\n",
      "    ->  Limit  (cost=0.72..518.64 rows=1 width=16) (actual time=0.017..0.017 rows=0 loops=999996)\n",
      "          ->  GroupAggregate  (cost=0.72..518.64 rows=1 width=16) (actual time=0.017..0.017 rows=0 loops=999996)\n",
      "                Group Key: u0_1.author_id\n",
      "                ->  Nested Loop Left Join  (cost=0.72..468.35 rows=10056 width=16) (actual time=0.003..0.013 rows=20 loops=999996)\n",
      "                      ->  Index Scan using books_book_author_id_8b91747b on books_book u0_1  (cost=0.28..8.29 rows=1 width=16) (actual time=0.001..0.001 rows=0 loops=999996)\n",
      "                            Index Cond: (author_id = books_person.id)\n",
      "                      ->  Index Scan using books_book_readers_book_id_2c3eaf52 on books_book_readers u2  (cost=0.44..357.28 rows=10277 width=16) (actual time=0.385..3.961 rows=10056 loops=2000)\n",
      "                            Index Cond: (book_id = u0_1.id)\n",
      "Planning Time: 2.937 ms\n",
      "Execution Time: 20516.771 ms\n"
     ]
    }
   ],
   "source": [
    "from django.db.models import Count, OuterRef\n",
    "from django.contrib.postgres.expressions import ArraySubquery\n",
    "\n",
    "writings_subquery = Book.objects.filter(author_id=OuterRef(\"id\")).values(\"title\")\n",
    "readers_subquery = (\n",
    "    Book.objects.filter(author_id=OuterRef(\"id\"))\n",
    "    .values(\"author_id\")\n",
    "    .values(count=Count(\"readers__id\"))[:1]\n",
    ")\n",
    "writers_stats_qs = Person.objects.annotate(\n",
    "    writings_title=ArraySubquery(writings_subquery), readers_count=readers_subquery\n",
    ").values_list(\"name\", \"bio\", \"writings_title\", \"readers_count\")\n",
    "\n",
    "reset_queries()\n",
    "\n",
    "writers_stats = writers_stats_qs.all()\n",
    "\n",
    "print(connection.queries)\n",
    "print(\"query duration\", sum(float(query[\"time\"]) for query in connection.queries))\n",
    "print(\"pg explain analyze:\", writers_stats_qs.explain(ANALYZE=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Your RAM\n",
    "We first need a small tooling to measure the RAM consumed by our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from time import sleep\n",
    "\n",
    "import psutil\n",
    "\n",
    "\n",
    "def measure_ram_consumption(function_to_audit):\n",
    "    \"\"\"Output the RAM consumption of the function passed as parameter\"\"\"\n",
    "    initial_available_memory = psutil.virtual_memory().available\n",
    "    min_available_memory = initial_available_memory\n",
    "    is_running = True\n",
    "\n",
    "    class RamUsageThread(Thread):\n",
    "        def run(self) -> None:\n",
    "            nonlocal min_available_memory\n",
    "            while is_running:\n",
    "                min_available_memory = min(\n",
    "                    psutil.virtual_memory().available, min_available_memory\n",
    "                )\n",
    "                sleep(0.1)\n",
    "            return min_available_memory\n",
    "\n",
    "    ram_thread = RamUsageThread()\n",
    "    ram_thread.start()\n",
    "    function_to_audit()\n",
    "    is_running = False\n",
    "\n",
    "    print(\n",
    "        \"RAM consumption:\",\n",
    "        (initial_available_memory - min_available_memory) / 2**20,\n",
    "        \"MB\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate using the Queryset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM consumption: 1604.07421875 MB\n"
     ]
    }
   ],
   "source": [
    "def iter_over_persons():\n",
    "    for person in Person.objects.all():\n",
    "        pass\n",
    "\n",
    "\n",
    "measure_ram_consumption(iter_over_persons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate using an iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM consumption: 75.34765625 MB\n"
     ]
    }
   ],
   "source": [
    "def iter_over_persons_with_iterator():\n",
    "    for person in Person.objects.iterator():\n",
    "        pass\n",
    "\n",
    "\n",
    "measure_ram_consumption(iter_over_persons_with_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "4b191d08583b15c5c86d33e614b9f7b72f7d9e530f020d1f20483946b6900340"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
